{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key of this file\n",
    "  \n",
    "  1. Mini Project # 4 - Finding Waldo\n",
    "  \n",
    "  2. Finding Corners\n",
    "  \n",
    "  3. Feature Detection\n",
    "  \n",
    "  4. Histogram of Oriented Gradients\n",
    "   \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mini Project # 4 - Finding Waldo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "!pip install -U opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sabakat\\anaconda3\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\sabakat\\anaconda3\\lib\\site-packages (1.17.4)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a67293985f08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load input image and convert to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\WaldoBeach.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load input image and convert to grayscale\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\WaldoBeach.jpg')\n",
    "cv2.imshow('Where is Waldo?', image)\n",
    "cv2.waitKey(0)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load Template image\n",
    "template = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\waldo.jpg',0)\n",
    "\n",
    "result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "#Create Bounding Box\n",
    "top_left = max_loc\n",
    "bottom_right = (top_left[0] + 50, top_left[1] + 50)\n",
    "cv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n",
    "\n",
    "cv2.imshow('Where is Waldo?', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Template Matching\n",
    "\n",
    "There are a variety of methods to perform template matching, but in this case we are using the correlation coefficient which is specified by the flag **cv2.TM_CCOEFF.**\n",
    "\n",
    "So what exactly is the cv2.matchTemplate function doing?\n",
    "Essentially, this function takes a “sliding window” of our waldo query image and slides it across our puzzle image from left to right and top to bottom, one pixel at a time. Then, for each of these locations, we compute the correlation coefficient to determine how “good” or “bad” the match is. \n",
    "\n",
    "Regions with sufficiently high correlation can be considered “matches” for our waldo template.\n",
    "From there, all we need is a call to cv2.minMaxLoc on Line 22 to find where our “good” matches are.\n",
    "That’s really all there is to template matching!\n",
    "\n",
    "http://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding Corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image then grayscale\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\chess.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# The cornerHarris function requires the array datatype to be float32\n",
    "gray = np.float32(gray)\n",
    "\n",
    "harris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\n",
    "\n",
    "#We use dilation of the corner points to enlarge them\\\n",
    "kernel = np.ones((7,7),np.uint8)\n",
    "harris_corners = cv2.dilate(harris_corners, kernel, iterations = 2)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "image[harris_corners > 0.025 * harris_corners.max() ] = [255, 127, 127]\n",
    "\n",
    "cv2.imshow('Harris Corners', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harris Corner Detection is an algorithm developed in 1998 for corner detection  (http://www.bmva.org/bmvc/1988/avc-88-023.pdf) and works fairly well.\n",
    "\n",
    "**cv2.cornerHarris**(input image, block size, ksize, k)\n",
    "- Input image - should be grayscale and float32 type.\n",
    "- blockSize - the size of neighborhood considered for corner detection\n",
    "- ksize - aperture parameter of Sobel derivative used.\n",
    "- k - harris detector free parameter in the equation\n",
    "- **Output** – array of corner locations (x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Corner Detection using - Good Features to Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\chess.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# We specific the top 50 corners\n",
    "corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 150)\n",
    "\n",
    "for corner in corners:\n",
    "    x, y = corner[0]\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    cv2.rectangle(img,(x-10,y-10),(x+10,y+10),(0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Corners Found\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cv2.goodFeaturesToTrack**(input image, maxCorners, qualityLevel, minDistance)\n",
    "\n",
    "- Input Image - 8-bit or floating-point 32-bit, single-channel image.\n",
    "- maxCorners – Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned.\n",
    "- qualityLevel – Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure (smallest eigenvalue). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the  qualityLevel=0.01 , then all the corners with the quality - - measure less than 15 are rejected.\n",
    "- minDistance – Minimum possible Euclidean distance between the returned corners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The SIFT & SURF algorithms are patented by their respective creators, and while they are free to use in academic and research settings, you should technically be obtaining a license/permission from the creators if you are using them in a commercial (i.e. for-profit) application.\n",
    "\n",
    " ## SIFT\n",
    "\n",
    "http://www.inf.fu-berlin.de/lehre/SS09/CV/uebungen/uebung09/SIFT.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'SIFT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b1655ddf9de3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Create SIFT Feature Detector object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Detect key points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'SIFT'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SIFT Feature Detector object\n",
    "sift = cv2.SIFT()\n",
    "\n",
    "#Detect key points\n",
    "keypoints = sift.detect(gray, None)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich key points on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - SIFT', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURF\n",
    "\n",
    "http://www.vision.ee.ethz.ch/~surf/eccv06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SURF Feature Detector object\n",
    "surf = cv2.SURF()\n",
    "\n",
    "# Only features, whose hessian is larger than hessianThreshold are retained by the detector\n",
    "surf.hessianThreshold = 500\n",
    "keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
    "print \"Number of keypoints Detected: \", len(keypoints)\n",
    "\n",
    "# Draw rich key points on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - SURF', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST\n",
    "\n",
    "https://www.edwardrosten.com/work/rosten_2006_machine.pdf\n",
    "http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/AV1FeaturefromAcceleratedSegmentTest.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create FAST Detector object\n",
    "fast = cv2.FastFeatureDetector()\n",
    "\n",
    "# Obtain Key points, by default non max suppression is On\n",
    "# to turn off set fast.setBool('nonmaxSuppression', False)\n",
    "keypoints = fast.detect(gray, None)\n",
    "print \"Number of keypoints Detected: \", len(keypoints)\n",
    "\n",
    "# Draw rich keypoints on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - FAST', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRIEF\n",
    "\n",
    "http://cvlabwww.epfl.ch/~lepetit/papers/calonder_pami11.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create FAST detector object\n",
    "fast = cv2.FastFeatureDetector()\n",
    "\n",
    "# Create BRIEF extractor object\n",
    "brief = cv2.DescriptorExtractor_create(\"BRIEF\")\n",
    "\n",
    "# Determine key points\n",
    "keypoints = fast.detect(gray, None)\n",
    "\n",
    "# Obtain descriptors and new final keypoints using BRIEF\n",
    "keypoints, descriptors = brief.compute(gray, keypoints)\n",
    "print \"Number of keypoints Detected: \", len(keypoints)\n",
    "\n",
    "# Draw rich keypoints on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "                                    \n",
    "cv2.imshow('Feature Method - BRIEF', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oriented FAST and Rotated BRIEF (ORB)\n",
    "http://www.willowgarage.com/sites/default/files/orb_final.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create ORB object, we can specify the number of key points we desire\n",
    "orb = cv2.ORB()\n",
    "\n",
    "# Determine key points\n",
    "keypoints = orb.detect(gray, None)\n",
    "\n",
    "# Obtain the descriptors\n",
    "keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich keypoints on input image\n",
    "image = cv2.drawKeypoints(image, keypoints,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - ORB', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Histogram of Oriented Gradients\n",
    "\n",
    "- http://stackoverflow.com/questions/6090399/get-hog-image-features-from-opencv-python\n",
    "- http://www.juergenwiki.de/work/wiki/doku.php?id=public:hog_descriptor_computation_and_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image then grayscale\n",
    "image = cv2.imread('C:\\\\Users\\\\Sabakat\\\\Desktop\\\\images\\\\elephant.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Show original Image\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# h x w in pixels\n",
    "cell_size = (8, 8) \n",
    "\n",
    " # h x w in cells\n",
    "block_size = (2, 2) \n",
    "\n",
    "# number of orientation bins\n",
    "nbins = 9\n",
    "\n",
    "# Using OpenCV's HOG Descriptor\n",
    "# winSize is the size of the image cropped to a multiple of the cell size\n",
    "hog = cv2.HOGDescriptor(_winSize=(gray.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                  gray.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                        _blockSize=(block_size[1] * cell_size[1],\n",
    "                                    block_size[0] * cell_size[0]),\n",
    "                        _blockStride=(cell_size[1], cell_size[0]),\n",
    "                        _cellSize=(cell_size[1], cell_size[0]),\n",
    "                        _nbins=nbins)\n",
    "\n",
    "# Create numpy array shape which we use to create hog_feats\n",
    "n_cells = (gray.shape[0] // cell_size[0], gray.shape[1] // cell_size[1])\n",
    "\n",
    "# We index blocks by rows first.\n",
    "# hog_feats now contains the gradient amplitudes for each direction,\n",
    "# for each cell of its group for each group. Indexing is by rows then columns.\n",
    "hog_feats = hog.compute(gray).reshape(n_cells[1] - block_size[1] + 1,\n",
    "                        n_cells[0] - block_size[0] + 1,\n",
    "                        block_size[0], block_size[1], nbins).transpose((1, 0, 2, 3, 4))  \n",
    "\n",
    "# Create our gradients array with nbin dimensions to store gradient orientations \n",
    "gradients = np.zeros((n_cells[0], n_cells[1], nbins))\n",
    "\n",
    "# Create array of dimensions \n",
    "cell_count = np.full((n_cells[0], n_cells[1], 1), 0, dtype=int)\n",
    "\n",
    "# Block Normalization\n",
    "for off_y in range(block_size[0]):\n",
    "    for off_x in range(block_size[1]):\n",
    "        gradients[off_y:n_cells[0] - block_size[0] + off_y + 1,\n",
    "                  off_x:n_cells[1] - block_size[1] + off_x + 1] += \\\n",
    "            hog_feats[:, :, off_y, off_x, :]\n",
    "        cell_count[off_y:n_cells[0] - block_size[0] + off_y + 1,\n",
    "                   off_x:n_cells[1] - block_size[1] + off_x + 1] += 1\n",
    "\n",
    "# Average gradients\n",
    "gradients /= cell_count\n",
    "\n",
    "# Plot HOGs using Matplotlib\n",
    "# angle is 360 / nbins * direction\n",
    "color_bins = 5\n",
    "plt.pcolor(gradients[:, :, color_bins])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:352: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8e1e8b753ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Our Face Extractor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#13 is the Enter Key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:352: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
